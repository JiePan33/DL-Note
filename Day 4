 向量的长度通常称为向量的维度（dimension）。与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度。
 维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。 为了清楚起见，我们在此明确一下： 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。
 作为方阵的一种特殊类型，对称矩阵（symmetric matrix）等于其转置。
 B = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
B

<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [2, 0, 4],
       [3, 4, 5]], dtype=int32)>
       

B == tf.transpose(B)
<tf.Tensor: shape=(3, 3), dtype=bool, numpy=
array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True]])>
       
当我们开始处理图像时，张量将变得更加重要，图像以维数组形式出现， 其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。
X = tf.reshape(tf.range(24), (2, 3, 4))
X

<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=
array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]],

       [[12, 13, 14, 15],
        [16, 17, 18, 19],
        [20, 21, 22, 23]]], dtype=int32)>
        
具体而言，两个矩阵的按元素乘法称为Hadamard积（Hadamard product）。
A * B
<tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[  0.,   1.,   4.,   9.],
       [ 16.,  25.,  36.,  49.],
       [ 64.,  81., 100., 121.],
       [144., 169., 196., 225.],
       [256., 289., 324., 361.]], dtype=float32)>
       
将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。
a = 2
X = tf.reshape(tf.range(24), (2, 3, 4))
a + X, (a * X).shape
(<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=
 array([[[ 2,  3,  4,  5],
         [ 6,  7,  8,  9],
         [10, 11, 12, 13]],

        [[14, 15, 16, 17],
         [18, 19, 20, 21],
         [22, 23, 24, 25]]], dtype=int32)>,
 TensorShape([2, 3, 4]))
 
 
 降维
 我们可以对任意张量进行的一个有用的操作是计算其元素的和。 在数学表示法中，我们使用符号表示求和。 为了表示长度为的向量中元素的总和，可以记为
。 在代码中，我们可以调用计算求和的函数
x = tf.range(4, dtype=tf.float32)
x, tf.reduce_sum(x)

(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,
 <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)
 
 默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），我们可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。
A_sum_axis0 = tf.reduce_sum(A, axis=0)
A_sum_axis0, A_sum_axis0.shape

(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)>,
 TensorShape([4]))
 
指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。
A_sum_axis1 = tf.reduce_sum(A, axis=1)
A_sum_axis1, A_sum_axis1.shape

(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)>,
 TensorShape([5]))
 
 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。
 tf.reduce_sum(A, axis=[0, 1])  # Sameastf.reduce_sum(A)
 
 <tf.Tensor: shape=(), dtype=float32, numpy=190.0>
 
 
 一个与求和相关的量是平均值（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。
 tf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()
 
 
 (<tf.Tensor: shape=(), dtype=float32, numpy=9.5>,
 <tf.Tensor: shape=(), dtype=float32, numpy=9.5>)
 
 
 
 同样，计算平均值的函数也可以沿指定轴降低张量的维度。
 tf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0]
 (<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>,
 <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>)
 
 通过axis来决定根据哪条轴进行操作。
 
 
 如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），我们可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度。
 tf.cumsum(A, axis=0)
 
 <tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  6.,  8., 10.],
       [12., 15., 18., 21.],
       [24., 28., 32., 36.],
       [40., 45., 50., 55.]], dtype=float32)>
       
       
我们已经学习了按元素操作、求和及平均值。 另一个最基本的操作之一是点积。 给定两个向量
， 它们的点积（dot product）（或） 是相同位置的按元素乘积的和
y = tf.ones(4, dtype=tf.float32)
x, y, tf.tensordot(x, y, axes=1)

(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,
 <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>,
 <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)
 
 
 注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积：
 tf.reduce_sum(x * y) （same as （x, y, tf.tensordot(x, y, axes=1)））
 
 当权重为非负数且和为1（即
）时， 点积表示加权平均（weighted average）。 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。



在代码中使用张量表示矩阵-向量积，我们使用与点积相同的matvec函数。 当我们为矩阵A和向量x调用tf.linalg.matvec(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。
A.shape, x.shape, tf.linalg.matvec(A, x)

(TensorShape([5, 4]),
 TensorShape([4]),
 <tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 14.,  38.,  62.,  86., 110.], dtype=float32)>)
 
 
 
 我们可以将矩阵-矩阵乘法看作是简单地执行次矩阵-向量积，并将结果拼接在一起，形成一个矩阵。 在下面的代码中，我们在A和B上执行矩阵乘法。 这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。 两者相乘后，我们得到了一个5行3列的矩阵。
 B = tf.ones((4, 3), tf.float32)
tf.matmul(A, B)

<tf.Tensor: shape=(5, 3), dtype=float32, numpy=
array([[ 6.,  6.,  6.],
       [22., 22., 22.],
       [38., 38., 38.],
       [54., 54., 54.],
       [70., 70., 70.]], dtype=float32)>
       
矩阵-矩阵乘法可以简单地称为矩阵乘法，不应与”Hadamard积”混淆。


线性代数中最有用的一些运算符是范数（norm）。 非正式地说，一个向量的范数告诉我们一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。
事实上，欧几里得距离是一个L2范数： 假设维向量中的元素是x1,x2.....xn，其L2范数是向量元素平方和的平方根。
u = tf.constant([3.0, -4.0])
tf.norm(u)

<tf.Tensor: shape=(), dtype=float32, numpy=5.0>


在深度学习中，我们更经常地使用L2范数的平方。 你还会经常遇到L1范数，它表示为向量元素的绝对值之和。
与L2范数相比，L1范数受异常值的影响较小。 为了计算L1范数，我们将绝对值函数和按元素求和组合起来。
tf.reduce_sum(tf.abs(u))

<tf.Tensor: shape=(), dtype=float32, numpy=7.0>


类似于向量的L2范数，矩阵的Frobenius范数（Frobenius norm）是矩阵元素平方和的平方根。
Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的L2范数。 调用以下函数将计算矩阵的Frobenius范数。
tf.norm(tf.ones((4, 9)))

<tf.Tensor: shape=(), dtype=float32, numpy=6.0>


范数和目标
在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。


2.3.12. 小结
标量、向量、矩阵和张量是线性代数中的基本数学对象。

向量泛化自标量，矩阵泛化自向量。

标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。

一个张量可以通过sum和mean沿指定的轴降低维度。

两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。

在深度学习中，我们经常使用范数，如L1范数、L2范数和Frobenius范数。

我们可以对标量、向量、矩阵和张量执行各种操作。
